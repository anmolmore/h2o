{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>1 hour 36 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Kolkata</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.20.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>24 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_anmol_i32c5d</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>429.3 Mb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         1 hour 36 mins\n",
       "H2O cluster timezone:       Asia/Kolkata\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.20.0.5\n",
       "H2O cluster version age:    24 days\n",
       "H2O cluster name:           H2O_from_python_anmol_i32c5d\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    429.3 Mb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anmol/anaconda3/lib/python3.6/site-packages/h2o/utils/shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Total number of rows: 21613. Number of rows in train data set: 19462. Number of rows in validation data set: 0. Number of rows in  test data set 2151\n",
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "364369.34656071867\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "125650.06492025779\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "117960.13139155704\n",
      "deeplearning Model Build progress: |██████████████████████████████████████| 100%\n",
      "119512.45347264563\n",
      "stackedensemble Model Build progress: |███████████████████████████████████| 100%\n",
      "model performance on test for Neural Network:\n",
      "\n",
      "ModelMetricsRegression: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 13418958450.001537\n",
      "RMSE: 115840.22811614943\n",
      "MAE: 68646.9215215741\n",
      "RMSLE: 0.17720672728689413\n",
      "Mean Residual Deviance: 13418958450.001537\n",
      "\n",
      "model performance on test for Gradient Boosting:\n",
      "\n",
      "ModelMetricsRegression: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 17540412782.698\n",
      "RMSE: 132440.22343192418\n",
      "MAE: 64670.20924487638\n",
      "RMSLE: 0.16405326120956232\n",
      "Mean Residual Deviance: 17540412782.698\n",
      "\n",
      "model performance on test for Random Forest:\n",
      "\n",
      "ModelMetricsRegression: drf\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 17209483904.41525\n",
      "RMSE: 131184.92254986946\n",
      "MAE: 69204.16371956353\n",
      "RMSLE: 0.16975204025113966\n",
      "Mean Residual Deviance: 17209483904.41525\n",
      "\n",
      "model performance on test for GLM:\n",
      "\n",
      "ModelMetricsRegressionGLM: glm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 130254165455.17621\n",
      "RMSE: 360907.4195069647\n",
      "MAE: 235454.30520514087\n",
      "RMSLE: 0.5493003715628905\n",
      "R^2: 0.020130217555998975\n",
      "Mean Residual Deviance: 130254165455.17621\n",
      "Null degrees of freedom: 2150\n",
      "Residual degrees of freedom: 2063\n",
      "Null deviance: 285935013037402.7\n",
      "Residual deviance: 280176709894084.03\n",
      "AIC: 61332.28634303467\n",
      "\n",
      "model performance on test for Stacked Ensemble:\n",
      "\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 130370027720.42255\n",
      "RMSE: 361067.8990445184\n",
      "MAE: 235885.72213626606\n",
      "RMSLE: 0.550165251590505\n",
      "R^2: 0.019258614469497504\n",
      "Mean Residual Deviance: 130370027720.42255\n",
      "Null degrees of freedom: 2150\n",
      "Residual degrees of freedom: 2146\n",
      "Null deviance: 285935013037402.7\n",
      "Residual deviance: 280425929626628.9\n",
      "AIC: 61168.198826645086\n",
      "\n",
      "GLM    360907.419507\n",
      "RF     131184.922550\n",
      "GBM    132440.223432\n",
      "NN     115840.228116\n",
      "SE     361067.899045\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# all model saving and loading, grid search is removed from this script\n",
    "# this script only contains the code to load the data and create the models and check the RMSE scores of the models.\n",
    "# model training time is about 10 minutes in total for 4 models\n",
    "# the suprising finding is that the stacked ensemble does not give a good performance at all\n",
    "# this is unexpected - probably I made a mistake when creating the stacked model - I could not find it though.\n",
    "# so if you find it, please let me know.\n",
    "# the good news though is that I could find one model whose RMSE is below the 123000 threshold for cross validation and test data\n",
    "# this model is a Neural Network.\n",
    "#  \n",
    "# so overall I have fulfilled the exercise even though the ensemble model is not working as expected.\n",
    "import h2o\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "h2o.init()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "url = 'http://coursera.h2o.ai/house_data.3487.csv'\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "data = h2o.import_file(url)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# problem is that date is of type enum and not a date type\n",
    "# so we need to convert from enum to string and cut out the T000000 and then cast back to date\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "import datetime\n",
    "# using datetime for converting string to datetime: datetime.datetime.strptime('20141013', \"%Y%m%d\")\n",
    "# using datetime for converting datetime to string in a format that is h2o compatible: dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# create new column real_date which is of type time\n",
    "data['real_date']=h2o.H2OFrame(data[\"date\"].ascharacter().as_data_frame().applymap(lambda x: datetime.datetime.strptime(x[0:8], \"%Y%m%d\").strftime('%Y-%m-%d')))\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# creating additional column year\n",
    "data['year']=data['real_date'].year()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# creating additional column month\n",
    "data['month']=data['real_date'].month()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "data['zip_enum'] = data['zipcode'].asfactor()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# splitting data set into train an test data set according to split ration 90% for train and 10% for test, seed is 123\n",
    "train, test = data.split_frame([0.9], seed = 123)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "#Total number of rows: 21613. Number of rows in train data set: 19462. Number of rows in validation data set: 0. Number of rows in  test data set 2151\n",
    "print(\"Total number of rows: {}. Number of rows in train data set: {}. Number of rows in validation data set: {}. Number of rows in  test data set {}\".format(data.nrows, train.nrows, 0, test.nrows))\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "# RMSE goal is below 123000\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "y = \"price\"\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "ignore_fields = [y, \"date\", \"id\", \"zipcode\"]\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "xAll = set(data.names) - set(ignore_fields)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "# THE FOLLOWING columns are used for all models\n",
    "#{'bathrooms',\n",
    "# 'bedrooms',\n",
    "# 'condition',\n",
    "# 'floors',\n",
    "# 'grade',\n",
    "# 'lat',\n",
    "# 'long',\n",
    "# 'month',\n",
    "# 'real_date',\n",
    "# 'sqft_above',\n",
    "# 'sqft_basement',\n",
    "# 'sqft_living',\n",
    "# 'sqft_living15',\n",
    "# 'sqft_lot',\n",
    "# 'sqft_lot15',\n",
    "# 'view',\n",
    "# 'waterfront',\n",
    "# 'year',\n",
    "# 'yr_built',\n",
    "# 'yr_renovated',\n",
    "# 'zip_enum'}\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# Now generating 4 models and then create an ensemble\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# nfolds for crossvalidation for all models\n",
    "nfolds=7\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# GLM model\n",
    "# training time is 3 seconds\n",
    "mGLMs = H2OGeneralizedLinearEstimator(\n",
    "    family = \"gaussian\", # default is gaussian\n",
    "    model_id = \"mGLMs\",\n",
    "    nfolds=nfolds,\n",
    "    fold_assignment = \"Modulo\",\n",
    "    keep_cross_validation_predictions=True,\n",
    "    seed=50,\n",
    "    )\n",
    "mGLMs.train(xAll, y, train)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "# RMSE is of GLM is 364369 - that is bad\n",
    "print(mGLMs.rmse(xval=True))\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "# Random forest model\n",
    "# training time is 3 minutes\n",
    "mRFs = H2ORandomForestEstimator(\n",
    "    model_id = \"mRFs\",\n",
    "    nfolds=nfolds,\n",
    "    fold_assignment = \"Modulo\",\n",
    "    keep_cross_validation_predictions=True,\n",
    "    seed=50,\n",
    "    stopping_metric = \"rmse\",\n",
    ")\n",
    "mRFs.train(xAll, y, train)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "#RMSE of RF is 125650 - much better but above goal\n",
    "print(mRFs.rmse(xval=True))\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "# Gradient Boosting \n",
    "# creating a GBM with parameters that were discovered with grid search - train with crossvalidation\n",
    "# training time is 1 minute 37 sec\n",
    "mGBMs = H2OGradientBoostingEstimator(\n",
    "    model_id = \"mGBMs\",\n",
    "    nfolds=nfolds,\n",
    "    fold_assignment = \"Modulo\",\n",
    "    keep_cross_validation_predictions=True,\n",
    "    learn_rate = 0.1,\n",
    "    max_depth = 8,\n",
    "    nbins_cats = 2000,\n",
    "    nbins_top_level = 2000,\n",
    "    ntrees = 100,\n",
    "    seed = 50,\n",
    "    stopping_metric = \"rmse\"\n",
    "    )\n",
    "mGBMs.train(xAll, y, train)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "#RMSE of GBM is 117960 that is meeting the goal\n",
    "print(mGBMs.rmse(xval=True))\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "# for NN the following parameters were discovered via grid search\n",
    "#    distribution epochs     hidden      l1      l2   rate  \\\n",
    "#0           gamma  200.0   [10, 10]   0.001  1.0E-4   0.01   \n",
    "#1           gamma  200.0  [200, 10]  1.0E-5  1.0E-4   0.01 \n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "# Neural network model\n",
    "mNNs = H2ODeepLearningEstimator(\n",
    "    epochs=200,\n",
    "    hidden=[10,10],\n",
    "    l1 = 0.001,\n",
    "    l2 = 0.0001,\n",
    "    rate = 0.01,\n",
    "    activation=\"rectifier\",\n",
    "    seed=50,\n",
    "    model_id = \"mNNs\",\n",
    "    nfolds=nfolds,\n",
    "    stopping_metric=\"rmse\",\n",
    "    fold_assignment = \"Modulo\",\n",
    "    keep_cross_validation_predictions=True)\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "# training time is 5 minutes for NN\n",
    "mNNs.train(xAll, y, train)\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "# RMSE of NN is 119069\n",
    "print(mNNs.rmse(xval=True))\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "models = [mNNs, mGBMs, mRFs, mGLMs]\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "mSE= H2OStackedEnsembleEstimator(model_id = 'mSE', base_models= models, seed=50)\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "mSE.train(xAll, y, train)\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "# the stacked ensemble has a terrible performance even on the training data set - rmse is 363773\n",
    "# which is about as bad as the GLM.\n",
    "# the stacked ensemble does not nearly give the performance of the NN or the GBM or the RF.\n",
    "# something must be wrong...\n",
    "mSE.model_performance(train)\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "# the neural network has a RMSE of 117692 on the test data and thus meets the goal.\n",
    "# the NN meets the goal for cross validation and test data\n",
    "# mission accomplished\n",
    "print(\"model performance on test for Neural Network:\")\n",
    "print(mNNs.model_performance(test))\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "# the GBM does have a relative high RMSE of 132440 on the test data and does not meet the goal. The GBM suffers from overfitting.\n",
    "print(\"model performance on test for Gradient Boosting:\")\n",
    "print(mGBMs.model_performance(test))\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "# the RF scores better than the GBM. The RF has an RMSE of 131184 on the test data and does not meet the goal\n",
    "print(\"model performance on test for Random Forest:\")\n",
    "print(mRFs.model_performance(test))\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "# the GLM is the worst base model. The RMSE is 360907 on the test data and does not meet the goal. Very bad result. Model has very high bias.\n",
    "print(\"model performance on test for GLM:\")\n",
    "print(mGLMs.model_performance(test))\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "# the stacked ensemble is even worse then the worst base model (GLM). The stacked ensemble has RMSE of 361066 on the test data and does not meet the goal.\n",
    "# this result is not expected, it is not logical. Something must be wrong. Not sure what. It seems that the stacked ensemble\n",
    "# optimized for high RMSE and not low RMSE.\n",
    "print(\"model performance on test for Stacked Ensemble:\")\n",
    "print(mSE.model_performance(test))\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "all_models=[mGLMs, mRFs, mGBMs, mNNs, mSE]\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "names = ['GLM', 'RF',\"GBM\", 'NN','SE']\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "test_perf = list(map(lambda x: x.model_performance(test), all_models))\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "# Summary of the RMSE on the test data - as you can see the NN is below the threshold of 123000\n",
    "print(pd.Series(map(lambda x: x.rmse(), test_perf), names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
